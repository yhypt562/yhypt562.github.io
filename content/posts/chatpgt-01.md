---
title: "ChatGPT原理——驾校“AI教练”教导学员"
date: 2023-07-27T08:50:29+08:00
draft: false
---


最近ChatGPT是非常火啊，我一个“爱学习的”大码程序员都怦然心动，来蹭热度了。我赶紧就把 OpenAI 以往的 GPT-N 系列论文又翻出来，认真领会大语言模型（Large Language Model）的强大之处。经过一个多月的认真研究，查阅了几十篇相关的论文，终于在写这篇文章之前，熟练掌握了26个英文字母。

前段时间ChatGPT火起来后，就去看过很多大V的文章，简单了解了这个AI是咋回事。大部分人都在说GPT还是有很多不足，例如：

1. 回答无效问题：它只是根据之前的训练数据进行模式匹配和预测，随机抽取概率高的文字来回答，而没有真正理解人类想要的答案。
2. 缺乏可解释性：它很难进行深入的逻辑推理或解决需要多步骤操作的问题。因此，对于这类问题，GPT-3的回答可能不准确或含糊。
3. 提供虚假回答：它的训练数据中可能包含错误、虚假或误导性的信息，因此它可能会生成不准确或有误导性的回答。
4. 内容偏见问题：如果在训练数据中接触到偏见或歧视性的内容，它可能在生成回答时重复或放大这些偏见。这可能导致生成具有偏见或不公平观点的回答。

这些问题是如何产生的呢？下面我们带着问题，一步一步来看ChatGPT具体如何来做的。

## 为什么会有这样的问题？——无监督预训练（网上自学）

首先我们来看看为什么会出现这些问题，ChatGPT的核心是LLM，也就是大家常说到的大语言模型，而这种模型的核心就是为给定文本计算出后面出现的字的概率，举个通熟易懂的例子，如下：

![img](https://picx.zhimg.com/80/v2-5b4140ede9714119959b534d9337311d_720w.png?source=d16d100b)



编辑切换为居中

LLM大语言模型特点

输入一句“隔壁老”，它后面可能的下一个字是“王”、“张”或“李”。因为在前面的学习训练的结果中，这些字词出现的概率相对较高；语言模型实际上能够评估给定先前序列的每个可能词的可能性。例如隔壁老王，应该概率会比较高。回家看看你家有几个隔壁老王。哈哈😃

我再举几个例子：

> 寡妇门前_________             大概率会是：“寡妇门前是非多” 天使的脸蛋，魔鬼的_____  大概率会是：“魔鬼的身材”

那我们LLM就只能做文字接龙了吗？那这样的模型是不是就只有这一点价值了呢？也不能这样说，可以通过一些小技巧，也是可以得到一些答案的。例如下面的场景：

> 请问“四川最高的山是哪一座？”让GPT给出这句话后面的一个字：“贡”；然后再让GPT给出“四川最高的山是哪一座？贡”这句话的的后面一个字，可能是：“嘎”；接着让GPT给出“四川最高的山是哪一座？贡嘎”后面一个字，可能大概率就是“山”了。

![img](https://pic1.zhimg.com/80/v2-12db090bf98113b8dbad1d45915c916a_720w.png?source=d16d100b)



编辑切换为居中

提问回答过程

> 经过这样一个过程，还是有很大概率得出人类想要的答案的。

## ChatGPT如何改进的

前面提到了GPT是一个预测后续概率的模型，GPT也不知道哪些回答是人类想要的。比如问 GPT “四川省最高的山是哪座山？”，可能的回答就有多种，例如：“我也不知道，你能告诉我吗”、“贡嘎山”、“四姑娘山”或“你猜猜看” 都是上下文通顺的回答，但很显然 “贡嘎山”才 是更符合人类期望的回答。对于这种情况，GPT就只能随机选择一个或是几个答案了。所以OpenAI的方案是采用RLHF（人类反馈的强化学习）来进行训练，具体包括以下三个不同的步骤：

![img](https://pic1.zhimg.com/80/v2-a3f0b9d0641739699fcd8befbbc29331_720w.png?source=d16d100b)



编辑切换为居中

ChatGPT训练步骤

具体步骤：

1. 收集数据并进行监督学习（“老司机”掌握方向）
2. 收集对比数据并训练奖励模型（给AI找个“教练”）
3. 使用 PPO 强化学习算法针对奖励模型进行优化（AI“教练”指导 AI）

### 第一步：“老司机”掌握方向（有监督训练初始模型）

ChatGPT前一阶段，是采用无监督训练的，采用了网上的没有经过人类标记处理的数据，直接对ChatGPT进行训练。这样得到的模型，由于这些数据的来源五花八门，造成了回答不能满足人类需求的情况。

为了解决这种情况，研究人员让人类“AI培训师”从问题集中的抽取一些问题，丢给GPT3.5，让它给出答案。“AI培训师”再根据GPT3.5给出答案，然后进行人工修正，再把这些人类修正认可的问题和答案丢给 GPT3.5去学习。通过这种有训练的方法，我们就得到一个简易版的 ChatGPT 模型。也就是SFT，三伏天模型。😄

![img](https://picx.zhimg.com/80/v2-971b91b5a610cd847e0d98bdbf764c27_720w.png?source=d16d100b)



编辑切换为居中

收集数据并进行监督学习过程

讲到这里，就有人会问了，“AI培训师”毕竟是少数，怎么能穷尽所有的问题呢？实际上只需要提供数万条数据给AI就行，因为 GPT3.5 本来就有能力生成出正确答案，只是它不知道哪些答案是人类真正需要的。

### 第二步：给AI找个“教练”（收集对比数据并训练奖励模型）

![img](https://picx.zhimg.com/80/v2-1e3922e1d8724a54df765d9fc74e6b0e_720w.png?source=d16d100b)



编辑切换为居中

AI 教练

针对上一步，只要“AI培训师”人数足够多，理论上已经可以达到目的：训练出效果很好且符合人类偏好的模型。

若是全用人工，就算是能全部标记完，成本也是天文数字。这绝对不是一个好的解决方案。那又要通过什么办法来解决的呢？

大家记得前几年轰动一时的围棋人工智能 AlphaGo吧，它是通过海量的自我对弈优化模型，最终超越人类；那我们能不能让 GPT也通过大量对话练习提升其回答问题的能力呢？答案是可以，但还缺少一个来判断好坏的裁判或是老师。

对于AlphaGo下围棋来说，胜负是通过围棋的规则来决定；但 GPT 回答一个问题，那谁来告诉 GPT 回答的好坏呢？要是也用人来进行判断，那就回到了刚刚的问题，不可能全部答案都由人来进行判断。

如果有个能辨别 GPT 回答好坏的“教练模型”（即 Reward 模型），并按照人类的评分标准对 GPT 所给出的答案进行评分，那不就能帮助 GPT 的回答更加符合人类的偏好了么？

于是研究人员让 GPT 对特定问题给出多个答案，由人类来对这些答案的好坏做排序（相比直接给出答案，让人类做排序要简单的多）。基于这些评价数据，研究人员训练了一个符合人类评价标准/偏好的 Reward 模型。下图就是具体的步骤

![img](https://picx.zhimg.com/80/v2-623455230c49b9aa089dc32a8cde695c_720w.png?source=d16d100b)



编辑切换为居中

AI“教练”成长记

1. 取出一个问题，让第一步的模型生成若干回答
2. 人类对来自不同模型的回答做好坏排序
3. 基于这些数据来进行奖励模型的训练

### 第三步：AI“教练”指导 AI（强化学习优化模型）

在上一步已经训练出来了一名AI“教练”，现在我们就要让AI学员来开始练车（训练）了。“AI教练”可以对生成的回答进行打分，AI会根据评分来进行参数调整，以便下次得到更到的分数。然后一直重复这个过程，就得到了一名优秀的ChatGPT学员，可以顺利的出师了。

![img](https://picx.zhimg.com/80/v2-252d25ba7b4f58f379483ac2f9019d32_720w.png?source=d16d100b)



编辑切换为居中

AI教练指导AI学员 练车过程

## 总结

ChatGPT的训练过程就和考驾照类似，例如：

- 无监督预训练（网上自学）：驾校学员学车前，先去网上找找学车的注意事项，网上的消息总是五花八门的，而且还不全是正确的，一个问题有多种答案。
- “老司机”掌握方向（有监督训练初始模型）：学员缴费后，由驾校老师讲解开始科目的教学，给学员发交规及题库等资料，学员根据资料学习。
- 给AI找个“教练”（收集对比数据并训练奖励模型）：学员太多，教练感觉自己精力不够，教练就会对不同学员的的学习进行打分，根据分数排序，然后把这个数据提供给教练的助理，助理就知道了教练的打分偏好。
- AI“教练”指导 AI（强化学习优化模型）：就像是教练的助理，每天对学员学习情况打分，然后学员根据自己的分数，自行摸索改进。